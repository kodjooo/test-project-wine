## Общая структура
- `app/main.py` — точка входа, управляет запуском асинхронного пайплайна и сценарием обхода.
- `app/config.py` — загрузка конфигурации из переменных окружения и `config.json`, валидация через Pydantic.
- `app/crawler/` — управление Playwright (Chromium), обход категории, обработка пагинации, сбор ссылок на карточки.
- `app/parser/` — загрузка страниц товаров, извлечение исходных полей, подготовка данных к нормализации.
- `app/normalizer/` — нормализация числовых и текстовых значений, интеграция с LLM и fallback на локальную обработку.
- `app/media/` — загрузка изображений через FreeImage.host, вычисление SHA-256, кеширование соответствий хэш → direct URL.
- `app/sheets/` — upsert записей в Google Sheets, формирование формулы `=IMAGE()` и статусов new/updated/skipped.
- `app/state/` — слой работы с SQLite (таблицы `visited_urls`, `image_hashes`), обеспечивает идемпотентность.
- `app/telemetry/` — логирование, счётчики и итоговая сводка; опциональный веб-хук Telegram на будущее.

## Поток данных
1. `main` инициализирует конфигурацию, подключает хранилище состояния и сервисы.
2. `crawler` получает страницы категории, обрабатывает пагинацию, возвращает очередь ссылок товаров.
3. Для каждой карточки `parser` извлекает структуру `ProductRaw` (все поля в сыром виде плюс HTML-фрагменты разделов).
4. `normalizer` переводит данные в унифицированный `ProductNormalized`, вызывает LLM для сложных полей и списков сортов.
5. `media` гарантирует наличие изображения в FreeImage.host, возвращает прямой и viewer URL, а также хэш для повторного использования.
6. `sheets` обновляет или вставляет строку в таблицу, используя данные из нормализованной структуры и ссылки на изображение.
7. `state` синхронно обновляется после успешной обработки, `telemetry` фиксирует статистику по прогрессу и ошибкам.

## Конфигурация и секреты
- `.env` / `.env.example` — основные переменные окружения (URL категории, идентификаторы Google Sheets и параметры FreeImage.host, настройки Playwright, прокси).
- `config.json` — дополнительные селекторы и настройки пагинации (при необходимости изменяются после анализа DOM).
- Секреты сервисного аккаунта Google хранятся в `./secrets/sa.json` и монтируются в контейнер в режиме `ro`.
- LLM использует токен OpenAI через переменную `OPENAI_API_KEY`, конфигурация модели задаётся через `LLM_MODEL`.

## Хранилище
- SQLite-файл в каталоге `./state` (монтируется как volume в Docker).
- Таблица `visited_urls` — `product_url`, `updated_at`, `etag_hash`.
- Таблица `image_hashes` — `sha256`, `direct_url`, `viewer_url`, `thumb_url`, `original_url`, `updated_at`.
- `StateRepository` мигрирует прежнюю схему (`drive_file_id` / `public_url`) до создания индексов, поэтому существующий `state/pipeline.db` обновляется на лету без ручных действий.
- Возможность расширить схему телеметрией (например, `runs_history`) при необходимости.

## Внешние зависимости
- Playwright (Chromium) для обхода сайта и управления всплывающим подтверждением возраста.
- httpx + tenacity для скачивания изображений и повторов сетевых запросов.
- Google Sheets через `gspread` (сервисный аккаунт); загрузка изображений — через REST API FreeImage.host.
- OpenAI SDK для LLM-нормализации.

## Логи, телеметрия и устойчивость
- Базовый `logging` с уровнем INFO, расширяемый до JSON-формата (структурные логи) при необходимости.
- Счётчики: `pages_processed`, `products_total`, `inserted`, `updated`, `skipped`, `errors`.
- Повторы и таймауты: глобальные настройки в конфигурации (`REQUEST_DELAY_MS`, `MAX_RETRIES`, `NAVIGATION_TIMEOUT_MS`).
- Логирование краулера фиксирует найденные и отфильтрованные дубликаты карточек с указанием страницы и примеров URL.

## Тестирование
- Модульные тесты для парсера, нормализатора, слоя состояния (табличные тесты).
- Интеграционные тесты с моками Google API и OpenAI (через `pytest` + `respx` или фиксированные фэйковые клиенты).
- Снапшоты HTML в `tests/fixtures` для проверки устойчивости селекторов.

## Docker и запуск
- Базовый образ `python:3.11-slim`, установка Playwright и зависимостей через `requirements.txt`.
- Контейнер `scraper` в `docker-compose.yml` запускает `python -m app.main`, монтирует `./state` и `./secrets`.
- Параметры управления (категория, API ключи, прокси) задаются через `.env`.

## Этап 2 — Crawler
- `CategoryCrawler` создаёт отдельную страницу Playwright и обходит очередь URL (одна или несколько категорий из `CATEGORY_URLS`, либо одиночная `CATEGORY_URL`) с учётом посещённых страниц.
- Очередь пополняется за счёт ссылок пагинации (`a[href*='PAGEN_1=']`), адреса нормализуются через `urljoin`.
- Основной селектор карточек: `a[href^='/katalog/tovar/']`, позиции сохраняются в `ProductLink`.
- Метрики (`CategoryCrawlerMetrics`): количество страниц, общих и уникальных карточек.
- Пауза между страницами регулируется `REQUEST_DELAY_MS`, User-Agent выбирается из пула `Settings.choice_user_agent()`.

## Этап 3 — Parser
- `ProductPageParser` открывает карточку в новой вкладке Playwright, закрывает модалку 18+, ожидает `h1`.
- Парсинг выполняется с помощью `selectolax`: заголовок, артикул, бренд, страна, хлебные крошки и факты (`0.7 л`, `40 %` и т.д.).
- Секции `h4` сопоставляются с ключами (`tasting_notes`, `grapes`, `producer` и др.); содержимое хранится в `ProductSection`.
- Числа предварительно нормализуются (`extract_price_value`, `extract_float_with_unit`, `extract_abv_percent`).
- Изображения собираются из `<picture>/<img>`: анализируем `srcset`, выбираем URL с максимальным разрешением и используем его как `hero_image_url`.
- Результат возвращается в структуре `ProductRaw`, пригодной для последующей нормализации и записи в хранилища.

## Этап 4 — Normalizer и LLM
- `ProductNormalizer` принимает `ProductRaw`, возвращает `ProductNormalized` (готовый к записи в хранилище/Sheets).
- Нормализация выполняется эвристиками: преобразование цены, объёма, крепости, статуса наличия, вычисление возраста.
- Вызовы LLM (`LLMClient` на OpenAI) используются, если не удалось распарсить числовые значения или нужно очистить сложные секции.
- `ProductSection` хранит как очищенный текст, так и `raw_text`/`html` — при пустых значениях нормализатор отправляет HTML фрагмент в LLM.
- Метрики `NormalizerMetrics` фиксируют количество карточек, число вызовов LLM и ошибки, что позволит контролировать квоты.

## Этап 5 — Media Uploader
- `MediaUploader.ensure_image` пытается загрузить изображение напрямую по исходному URL через FreeImage.host; при отказе (hotlink) скачивает файл локально и отправляет бинарно.
- Ответ API парсится на `direct_url`, `viewer_url`, `thumb_url`; эти значения сохраняются вместе с SHA-256 в `image_hashes` и переиспользуются при повторных запусках.
- При отсутствии API-ключа работает деградированно: считает хэш и пропускает загрузку, чтобы пайплайн оставался идемпотентным.
- Реализованы ретраи с экспоненциальной паузой, отдельные таймауты connect/read берутся из `.env`.

- При повторном обращении модуль сначала ищет запись по оригинальному URL, затем — по хэшу, что позволяет избежать лишних выгрузок.

## Этап 6 — Sheets Writer
- `SheetsWriter` авторизуется в Google Sheets (сервисный аккаунт) и работает с листом `GSHEET_TAB`.
- Данные подготавливаются в `SheetRecord` по актуальной схеме колонок без служебных полей (`TIMESTAMP_UTC`, `SOURCE_CATEGORY_URL`, `PAGE_NUM`, `PRODUCT_ID`, `PRICE_CURRENCY`, `SKU`, `AWARDS`, `BREADCRUMBS` исключены).
- Upsert выполняется по `PRODUCT_URL`: поиск строки по колонке URL, обновление или добавление через batch-update/append.
- Формула `IMAGE_CELL` записывается как `=IMAGE(IMAGE_DIRECT_URL)`, сохраняем только прямой URL изображения для предпросмотра.
- Колонка `POSITION` хранит глобальный номер карточки; при старте пайплайн запрашивает максимальное значение и продолжает со следующего.
- При отсутствии сервисного аккаунта записи пропускаются (вернётся статус `skipped`), чтобы пайплайн мог выполняться локально без доступа.

## Этап 7 — State & Dedup
- `StateRepository` (SQLite) хранит таблицы `visited_urls` (product_url → etag_hash, image_sha) и `image_hashes` (sha256 → direct/viewer/thumb URL).
- Обновление выполняется через `INSERT ... ON CONFLICT DO UPDATE`, что гарантирует идемпотентность.
- Контрольная сумма карточки (`product_etag`) вычисляется хэшем от основных полей `ProductNormalized`, что позволяет пропускать неизменённые записи.
- Для изображений используется кеширование по SHA-256, что исключает повторные загрузки одинаковых картинок.
- Репозиторий управляет созданием директории `state/` и безопасно закрывает соединение после завершения пайплайна.

## Этап 8 — Телеметрия и эксплуатация
- Логирование централизовано через `logging.basicConfig`, индикаторы прогресса выводятся после завершения краулера, парсера, нормализатора и Sheets Writer.
- Подготовлены счётчики `inserted`, `updated`, `skipped` для выгрузки данных, что облегчает контроль качества прогонов.
- Docker-ориентированное окружение: базовый образ `mcr.microsoft.com/playwright/python:v1.45.0-jammy` содержит готовые браузеры Chromium и системные зависимости.
- Переменные окружения для таймаутов, задержек и путей вынесены в `.env`, что позволяет управлять нагрузкой без перекомпиляции образа.
- Все ключевые этапы пайплайна (парсинг, нормализация, загрузка изображений, запись в Sheets) выводят подробные INFO-логи с URL карточек и диагностикой ошибок/фолбэков.

## Этап 9 — Docker, тесты и документация
- `docker-compose.yml` поднимает сервис `scraper`, монтирует каталоги `state/` и `secrets/`, использует `.env` для конфигурации.
- README описывает подготовку окружения, запуск пайплайна/тестов через Docker Desktop и требования к развёртыванию на сервере.
- Автотесты (pytest) проверяют парсер, нормализатор и слой состояния; запуск `docker compose run --rm scraper pytest` встроен в рабочий цикл.
- Документация (requirements/architecture/plan) актуализирована: план этапов, архитектурные решения, бизнес-требования жильются в одном месте.

## Этап 8 — Телеметрия и устойчивость
- Логирование выполняется через стандартный `logging` с уровнем INFO, в `app/main.py` выводится сводка по каждому модулю.
- Управляемые задержки/ретраи конфигурируются через `.env` (`REQUEST_DELAY_MS`, `MAX_RETRIES`, `NAVIGATION_TIMEOUT_MS`).
- При отсутствии внешних секретов (Sheets/FreeImage/LLM) система автоматически переходит в «мягкий» режим, фиксируя `skipped`.
- Все сетевые операции (Playwright, httpx) обёрнуты в try/except; ошибки сохраняются в лог, пайплайн продолжает обработку.

## Этап 9 — Docker, тесты и документация
- Dockerfile основан на `mcr.microsoft.com/playwright/python:v1.45.0-jammy`, что гарантирует наличие браузеров и зависимостей Playwright.
- `docker-compose.yml` описывает сервис `scraper`, монтирует каталоги `state/` и `secrets/`, использует `.env` для конфигурации.
- README содержит инструкции по подготовке `.env`, запуску скрейпера через Docker Desktop, прогону тестов и развёртыванию на сервере.
- Автотесты (`pytest`) запускаются внутри контейнера и покрывают парсер, нормализатор и слой состояния.
